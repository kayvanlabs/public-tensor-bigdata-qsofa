% Provide sufficient details to allow the work to be reproduced by an independent researcher. Methods that are already published should be summarized, and indicated by a reference. If quoting directly from a previously published method, use quotation marks and also cite the source. Any modifications to existing methods should also be described.

A schematic of the methods used in this paper is presented in Fig.\ref{fig:schematic}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{schematic_v2.png}
    \caption{Schematic}
    \label{fig:schematic}
\end{figure}

% About dataset
\subsection*{Dataset} \label{sec:methods_dataset}

The retrospective dataset consisted of 1,803 unique individuals age $\geq$ 18 years with 3,516 unique encounters between 2013-2018 at Michigan Medicine. Individuals' characteristics are presented in Supplementary Table 1. The detailed inclusion/exclusion criteria for the dataset are provided in Supplementary Materials Section 1.4, but briefly, inclusion criteria selected for inpatient encounters with: ECG lead II waveforms at least 15 minutes in length and ICD9/10 codes for pneumonia, cellulitis, or urinary tract infection (UTI), excluding UTIs associated with catheters. Exclusion criteria included positive HIV status, solid organ or bone marrow transplant, and ongoing chemotherapy. These criteria created a dataset that did not specifically select for sepsis diagnosis, but instead focused on patients with an infection who were at risk to develop sepsis and septic shock. This dataset was selected from a Michigan Medicine biobank, whose data collection was approved by the institutional review board of University of Michigan. Informed consent was waived, as this was a retrospective study of previously collected and de-identified data, without direct involvement of human subjects and therefore no chance of physical harm or discomfort to the individuals being studied. Individuals reported their own sex and race/ethnicity, from categories defined by Michigan Medicine, and this information is included in Supplementary Table 1 to provide information on the population of this study.

This larger dataset was reduced by selecting for individuals who had EHR, ECG, and arterial line data available. In this study, EHR data included labs, medications, hourly fluid output, and vital signs. Because poor signal quality can result in false alarms \autocite{gambarotta_review_2016}, the ECG signal was reviewed automatically using Pan-Tompkins to identify QRS complexes \autocite{pantom_1985, matlab-pantom}. Upon collecting 10-minute signals for feature extraction, signals determined to be 50\% or more noise were discarded.

Change in qSOFA score was used to assign positive and negative classes for machine learning. Given an individual who meets one of the criteria for qSOFA, the model predicts whether the score will increase to $\geq$ 2, which Sepsis-3 deems as \quotes{likely to have poor outcomes} \autocite{sepsis-3}. This increase in qSOFA is considered the positive outcome in a learning context, because the patient meets at least 2 qSOFA criteria as defined by Sepsis-3 after the prediction gap. Thus, the negative outcome is qSOFA $<$ 2 after the prediction gap.

We tested prediction gaps of six and twelve hours. For a six-hour gap, there were 199 negative and 59 positive cases. For a twelve-hour gap, there were 189 negative and 37 positive cases.

\subsection*{Signal Processing} \label{sec:methods_sigproc}
For every sample, we collected the 10 minutes of signal occurring directly before the prediction gap for processing. This 10-minute signal was divided into 2 5-minute windows, and then preprocessed according the relevant sections below.

\subsubsection*{Arterial Line Data} \label{sec:methods_art_data}
Arterial line data was sampled at 120 Hz. We applied a third order Butterworth bandpass filter with cutoff frequencies 1.25 and 25 Hz  to remove artifacts. The \textit{BP\_Annotate} software package \autocite{laurin_bp_annotate_2017} annotated the signal. Following previous methodology, \autocite{luo_severity_2012, hernandez_multimodal_2021}, we extract features from the annotated signal: number of peaks, as well as the minimum, maximum, mean, median, and standard deviation (SD) of time between sequential systolic peaks, time between a systolic peak and its subsequent diastolic reading, relative amplitude between systolic peaks, and relative amplitude between a systolic peak and its subsequent diastolic reading.

\subsubsection*{Electrocardiogram Data} \label{sec:methods_ecg_data}
% About ECG, signal processing
ECG data consisted of four leads and was sampled at 240 Hz. We used lead II of the ECG, following previously established methods \autocite{belle_signal_2016}. A second order Butterworth bandpass filter with the cutoff frequencies 0.5 and 40 Hz removed noise and artifacts.

\subsubsection*{Taut String} \label{sec:methods_ts}
Peak-based and statistical features were calculated from the Taut String (TS) estimation \autocite{taut_string} of the ECG waveform. Others have previously used such features to detect hemodynamic instability \autocite{belle_signal_2016} and predict hemodynamic decompensation \autocite{hernandez_multimodal_2021, kim_prediction_2022}. TS estimation functions as follows. Given a discrete signal

$f = (f_0, f_1, ..., f_n)$,
for a fixed value $\epsilon > 0$, the TS estimate of $f$ is the unique function $g$ such that 

\begin{equation*}
    \norm{f - g}_{\infty} = \max\limits_{i}\{ \lvert f_i - g_i \rvert \} \leq \epsilon
\end{equation*}
and
\begin{equation*}
    \norm{D\left( g \right)}_{2} = \sqrt{\sum^{n - 1}_{i = 1} \left( g_{i + 1} - g_i \right)^2}
\end{equation*}
is minimal, with $D$ being the difference operator.

TS estimation was applied to the filtered ECG signal using the five values of the parameter $\epsilon$: 0.0100, 0.1575, 0.3050, 0.4525, and 0.6000. These values were selected from previous work \autocite{hernandez_multimodal_2021}. Six features were computed from each TS estimate of a 5-minute window and value of $\epsilon$. These features were: number of line segments, number of inflection segments, total variation of noise, total variation of denoised signal, power of denoised signal, and power of noise. This resulted in a tensor of size $2 \times 5 \times 6$ for each signal, where the modes of the tensor were window, $\epsilon$, feature. 

\subsection*{Electronic Health Record Data} \label{sec:methods_ehr}
We assigned an ordinal encoding to labs and cardiovascular infusions ranging from 0-4 or 0-3, respectively. A score of 1 indicates less severity and a score of 3 or 4, more severity. If a lab value had been recorded before the time of interest, this value was carried forward. We assigned a score of 0 to represent a missing value with no previous recordings. The Supplementary Materials Section 1.2 provides tables detailing these assignments. Vital signs and urine output were included, but not given an ordinal encoding. If vital signs or urine output were not reported in the time of interest, we carried forward the most recent known value.

We added a retrospective component for lab values, cardiovascular infusions, and vital signs where, in addition to the 10 minutes occurring before the prediction gap, we include four look-back periods. For the prediction gap of 6 hours, these look-back periods are increments of 4 hours; for the prediction gap of 12 hours, they are increments of 8 hours.

\subsection*{Feature Reduction with Tensor Methods} \label{sec:methods_tensor}

For each 10-minute ECG signal, 60 features were computed and arranged as a tensor of size $2\times5\times6$. For each 10-minute arterial line signal, 42 features were arranged as a tensor of size $2\times1\times21$, where the second mode, TS parameter $\epsilon$, was inflated to create a uniform presentation to the tensor reduction algorithms. Rather than treating this information as 60 or 42 feature vectors, we preserved the underlying tensor structure by using a tensor-based dimensionality reduction method, inspired by previous work \autocite{hernandez_multimodal_2021} and described below.

First, each tensor's underlying structure was determined. All $2 \times 5 \times 6$ ECG-feature tensors in the training set were stacked along the fourth mode, generating a new tensor of size $2 \times 5 \times 6 \times N$, where $N$ was the number of observations in the training set. Similarly, all $2 \times 1 \times 21$ arterial line-feature tensors were stacked along the fourth mode to generate a new tensor of size $2 \times 1 \times 21 \times N$.

\textit{Tensor Toolbox's} \autocite{tensor_toolbox_gitlab} Canonical Polyadic / Parallel Factors (CP) decomposition \autocite{kolda_tensors} was used to obtain the underlying structure of the tensors. A CP decomposition breaks the initial tensor down into a sum of rank-1 tensors, so it can be considered an extension of singular value decomposition to a higher order. 

In general, given a tensor

\begin{equation*}
    X = \mathbb{R}^{n_1 \times \dots \times n_d}
\end{equation*}
and a predetermined rank $r$, the CP decomposition gives a tensor
\begin{equation*}
    \hat{X} = \sum^{r}_{i=1} v_{1_{i}} \otimes \dots \otimes v_{d_{i}}
\end{equation*}
such that $\norm{X - \hat{X}}$ is minimized, where $\otimes$ denotes the Kronecker product. The multiplication of vectors $v_1, \dots v_d$ yields a component rank-1 tensor. Because the tensors used in this specific case are fourth-order, this can be written as:

\begin{equation*}
    X \approx \hat{X} = \sum^{r}_{i=1} a_i \otimes b_i \otimes c_i \otimes d_i
\end{equation*}

The vectors $a_1,\dots,a_r\in\mathbb{R}^{n_1}$, and so on, can be combined to form factor matrices, such as $A = [a_1, \dots, a_r]\in\mathbb{R}^{n_1\times r}$, and similarly for $B, C, D$. In this manner, each mode of the original tensor $X$ can be approximated by the product of these factor matrices, such as:

\begin{equation*}
    X_{(1)} \approx A \left( D \odot C \odot B \right)^{\top}
\end{equation*}
where $\odot$ denotes the Khatri-Rao product for the first mode. Because finding a CP decomposition is NP-hard \autocite{hillar_most_2013}, we used the Alternating Least Squares (ALS) heuristic method, which is an iterative algorithm to find the best approximation of $X$ for a given rank $r$ \autocite{kolda_tensors}.

The dataset was divided into an 75/25 split 100 times, and tensor reduction was performed on each of those splits. A fit score, defined as
\begin{equation*}
    \mathrm{fit} = 1 - \frac{\norm{X - \hat{X}}}{\norm{X}}
\end{equation*}
, was calculated to determine how well the reduced tensor approximated the original. This process was repeated 15 times, with the selected reduction being the one with the highest fit, or the first reduction with fit score equal to one, whichever occurred first. CP-ALS was run using rank values of 1-4.  

After applying CP-ALS to the training data, the resultant factor matrices $A$ and $B$ were retained, which related to the modes of the original tensor that were not the feature mode ($C$) or the patient encounter mode ($D$). 

With this process completed, for any given individual's third-order tensor $T$, a reduced set of features was extracted using the factor matrices computed from the training data. The feature vectors $c_{T,1}, \dots, c_{T,r}$ were computed via a least squares problem, where

\begin{equation*}
    \norm{T - \sum^{r}_{i=1} a_i \otimes b_i \otimes c_{T,i}}
\end{equation*}
is minimal. After computing the individual vectors, they were concatenated to create $C_T$, a feature matrix with a reduced set of features compared to matricization $T_{(3)}$ of the original tensor $T$ along the third mode.

\subsection*{Machine Learning} \label{sec:methods_ml}
When constructing training and test datasets, 75/25 splits were created based on individuals so that no individual would overlap between the training and test sets.

After extracting features, the three types of learning models used for training were Support Vector Machines (SVM) \autocite{cortes_support-vector_1995}, Random Forest (RF) \autocite{breiman_random_2001}, and Learning Using Concave and Convex Kernels (LUCCK) \autocite{sabeti_learning_2019}.

For all methods, the training phase consisted of 3-fold cross-validation (3FCV) on a 75/25 split of the data, where the test set was held and not used for training. The test set was presented to the three models generated from 3FCV to produce three sets of prediction scores. We computed the final prediction scores for the test set by taking the median of the three prediction scores, thus creating a voting system. This process was repeated 100 times to obtain mean and standard deviation values of model performance. 

A grid search selected optimal hyperparameters for each model using the validation fold in 3FCV. For RF, these hyperparameters included: number of trees, minimum leaf size, fraction of maximum number of splits, and number of predictors to sample. For SVM, grid search selected the best box constraint $C$ and kernel scale $\gamma$. Sequential minimal optimization \autocite{JMLR:v6:fan05a} was used for the optimization routine. For LUCCK, grid search selected optimal $\Lambda$ and $\Theta$ parameters. All grid searches used F1 score as the value to optimize. 

Different signal feature based models were tested using tensor reduction. The first, using only ECG data and presented in Figure \ref{fig:ecgonly}, was the most restricted model, assuming that both EHR and arterial line data were unavailable. This would apply to patients recently admitted, who would not have lab values or other EHR data available, and is also minimally invasive compared to having an arterial line in place. Next, a model trained on both ECG and arterial line features, presented in Figure \ref{fig:sigonly}, which was tested to determine if the invasive arterial line improved performance compared to only using ECG data. Lastly, a model trained on signal features alongside EHR data was built, presented in Figure \ref{fig:sigEHR}.

\subsection{Code Availability}
The underlying code for this study is available in on GitHub and can be accessed via this link: https://github.com/kayvanlabs/public-tensor-bigdata-qsofa. Our repository does not provide libraries created by others as to not violate their licenses. Additionally, the source code of LUCCK and Taut String methods cannot be shared due to proprietary reasons, but standalone Matlab executables are made available in the GitHub repository.